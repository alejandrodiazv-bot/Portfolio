# -*- coding: utf-8 -*-
"""Titanic (Taggle).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubvNMaNnO24pW6kt2NvnGwly6-TEuU58
"""

#Set de datos del "Titanic" en "Taggle"
import pandas as pd
datos = pd.read_csv('U4_04_train.csv')

datos.head()

datos.describe()

#Veremos cuantas personas sobrevivieron y cuantas no, usando
#la funcion "countplot" de "Seaborn"
import seaborn as sb
sb.countplot(x='Survived', data=datos, hue='Survived')

#El grafico anterior lo dividiremos en hombre y mujeres
#usando la funcion "hue"
sb.countplot(x='Survived', data=datos, hue='Sex')

#Podemos usar la funcion "isna" para
#conocer los datos vacios
datos.isna()

#Podemos contabilizar los datos
#vacios con la funcion "sum"
datos.isna().sum()

#Para no borrar los 177 datos de la columna "edad"
#esos datos vacios lo llenaremos con promedios
#y si borrar los datos de la columna "cabin"
#Usaremos la funcion "displot"
sb.displot(x='Age', data=datos)

#Del gráfico anterior, se ve que la mayoria esta
#entre los 15 y 35 años. Entonces
datos['Age']

#Para ver el promedio de edad, se
#utiliza la funcion "mean"
datos['Age'].mean()

#Como existe dropna para datos vacios
#existe fillna para datos vacios y para
#eso, llenaremos con la celda anterior
datos['Age'].fillna(datos['Age'].mean())

#Al hacer esto, no me modfica la
#columna 'Age' de los datos. Por
#eso, modificaremos esta columna
#de la sgte manera
datos['Age'] = datos['Age'].fillna(datos['Age'].mean())

#Si vuelvo a mostrar la columna 'Age', queda
#actualizada
datos['Age']

#Ahora, si vuelvo a mostrar
#datos.isna().sum(), queda
#que ya no hay datos vacios en
#'Age'
datos.isna().sum()

#Ahora, para la columna 'Cabin'
#quitamos esta con la funcion
#"drop", especificando la columna
# y el eje (Axis) que es 1
datos = datos.drop(['Cabin'], axis=1)

#Si ocupamos datos.isna().sum()
#de nuevo, queda que 'Cabin' ya
#se fue
datos.isna().sum()

#Para la columna 'Embarked'
#usaremos la funcion 'value_counts'
datos['Embarked'].value_counts()

#como solo hay 2 embarcados que
#no estan especificados. Entonces
#usaremos la funcion dropna()
datos = datos.dropna()

datos.isna().sum()

#Después, podemos usar esto:
datos.head()

#Quitaremos las columnas PassengerId, Name, Ticket
#Para eso, usaremos 'drop':
datos = datos.drop(['PassengerId','Name', 'Ticket'], axis=1)

datos.head()

#Vemos que las columnas Name, PassengerId, Ticket
#Han salido de la nuestra tabla
#Ahora transformremos la columna "Sex" en numero. Para
#eso, usaremos la funcion "get_dummies"
pd.get_dummies(datos['Sex'], dtype=int)

#Como es una eleccion binaria,
#la columna 'Sex. Para transformala
#en una sola columna, agregaremos a
#pd.get_dummies(datos['Sex']) la
#función dropfirst
dummies_sex = pd.get_dummies(datos['Sex'], dtype=int, drop_first = True )

#Esto se hace para evitar la llamada
#multicolinealidad, que hace es buscar
#una relacion entre cosas opuestas. En
#este caso, male con female

#Para unirlo a nuestros datos con la funcion
# "Join" y quitando la columna de 'Sex' con 'drop'
datos_nue = datos.drop(['Sex'], axis=1)

datos_nue = datos_nue.join(dummies_sex)

datos_nue

#Ahora, veremos si es relevante utilizar la columna
#'Embarked'
sb.countplot(x='Survived', data=datos_nue, hue='Embarked')

#Ahora haremos 'Dummies' de 'Embarked'
#usando la función 'get_dummies' y sacamos
#la tercera alternativa usando "drop_first"
dummies_embarked = pd.get_dummies(datos_nue['Embarked'], drop_first = True ,dtype=int)

#Ahora, se lo agragamos a nuestra tabla datos_nue
#y quitamos la columna 'Embarked'
datos_nue = datos_nue.drop(['Embarked'], axis=1)
datos_nue = datos_nue.join(dummies_embarked)

datos_nue.head()

#Usando 'heatmap' de la libreria 'seaborn',
#y colocando 'annot=True' para conocer los
#coef. de correlacion y 'cmap' para el color
#del coeficiente
sb.heatmap(datos_nue.corr(), annot=True, cmap='YlGnBu')

sb.countplot(x='Survived', data=datos_nue, hue='Pclass')

#Del gráfico anterior, Podemos ver que hay
#una correlación positiva entre'Survived' (los que sobrevivieron)
#y 'Fare'(más pagaron)
#Si eres hombre(male), tienes menos probabilidad de
#haber sobrevivido
#Si eres de Pclass 1, tienes más posibilidad de sobrevivir
#que Pclass 3
#Si no queda claro, haremos otra grafica con 'countplot'
sb.countplot(x='Survived', data=datos_nue, hue='Pclass')

#Ahora, comenzaremos el entrenamiento, definiendo
# 'x'e'y', usando la funcion 'drop'
X = datos_nue.drop(['Survived'], axis=1)
y = datos_nue['Survived']

#Despues, hacemos lo sgte, usando
#'sklearn.model_selection' y su función
#'test_test_split'
from sklearn.model_selection import train_test_split
X_ent, X_pru, y_ent, y_pru = train_test_split(X, y, test_size=.2)

#Primero, importaremos un arbol de decision
#desde 'sklearn' el clasificador del arbol
# de decisiones, porque este es una clasificacion
#Es decir, solo responde una pregunta de si o no
#Si fuera de Regresion, responderia con un
#numero
from sklearn.tree import DecisionTreeClassifier

modelo = DecisionTreeClassifier(max_depth=50)
modelo.fit(X_ent, y_ent)
predicciones = modelo.predict(X_pru)

# Al ver su exactitud, podemos
# ver que bajo con respecto al de
# Regresion Logistica
from sklearn.metrics import accuracy_score
accuracy_score(y_pru, predicciones)

# Copiamos lo anterior y usarmos un
#ciclo 'For' para ver que ocurre
#segun cada iteracion

from sklearn.tree import DecisionTreeClassifier

resultados = []
for i in range(1,15):
  modelo = DecisionTreeClassifier(max_depth=i)
  modelo.fit(X_ent, y_ent)
  predicciones = modelo.predict(X_pru)
  exactitud = accuracy_score(y_pru, predicciones)
  print(f'Resultado para {i} : {exactitud}')
  resultados.append(exactitud)

#Usando 'lineplot' de seaborn,
#para crear una grafica, tenemos

sb.lineplot(data=resultados)

#Ahora vemos que otros hiperparametros tiene

from sklearn.metrics import classification_report

print(classification_report(y_pru, predicciones))

#Para entender, mejor los datos, usaremos
#la matriz de confusion ('confusion_matrix')
from sklearn.metrics import confusion_matrix

pd.DataFrame(confusion_matrix(y_pru, predicciones), columns=['Pred: No','Pred: Si'], index=['Real: No','Real: Si'])

# Una de las ventajas de los arboles de
#decision es que se pueden graficar
#Para eso, importamos desde 'sklearn.tree'
#la funcion 'plot_tree'

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(100,80))

plot_tree(
    modelo,
    feature_names=X_ent.columns,
    class_names=['Murió', 'Vivió'],
    filled=True, label='none'
)







#Pues bien, ahora seguimos con
#las predicciones siguientes:
predicciones = modelo.predict(X_pru)

#Usaremos las metricas de sklearn
#de la sgte manera
from sklearn.metrics import accuracy_score
accuracy_score(y_pru, predicciones)

#Como los datos no estan balanceados, es
#decir, que la cantidad de sobrevivientes
#es igual que la cantidad de fallecidos
#entonces, usaremos:
from sklearn.metrics import classification_report
print(classification_report(y_pru, predicciones))

#Para entender, mejor los datos, usaremos
#la matriz de confusion ('confusion_matrix')
from sklearn.metrics import confusion_matrix
confusion_matrix(y_pru, predicciones)

#Para arreglar esta matriz, usaremos
#Dataframe desde 'Pandas'
#Se puede ver que 99 no se salvaron
#y 47 si se salvaron. Por otra parte
#se equivoco la prediccion, cuando asevero que
# 22 no se salvaron, siendo que si. Y en 10 ocasiones
# que la pred. decia que si se salvaron, cuando, en
# realidad, era que no
pd.DataFrame(confusion_matrix(y_pru, predicciones), columns=['Pred: No','Pred: Si'], index=['Real: No','Real: Si'])

#Ahora, definamos a una persona y hagamos
#una predicion sobre ella
X.head()

nueva_persona= [3, 35, 0, 0, 10, 1, 0, 0]
prediccion = modelo.predict([nueva_persona])
if prediccion[0] == 1:
     print('Sobreviviste')
else:
     print('No sobreviviste')