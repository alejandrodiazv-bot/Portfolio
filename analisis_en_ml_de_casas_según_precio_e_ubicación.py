# -*- coding: utf-8 -*-
"""Analisis en ML de Casas según precio e ubicación.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5qLLuXRVmeheP75-xM42QGaiYFVoYgh
"""

import pandas as pd
datos = pd.read_csv('housing.csv')

datos.head()   #uso de la funcion head para ver los primeros 5 datos

datos['ocean_proximity'].value_counts()
#funcion value_counts nos dice cuantos datos hay y cuantos de cada uno

datos.info()     #funcion info muestra distintas columnas que datos vienen vacios y de que tipo
                 #en la columna total_bedrooms vienen 200 datos vacios

datos.describe()    #la funcion describe nos da por cada una de las columnas nos da datos interesantes como por ej la cantidad de registros, cual es el promedio
                    #como tambien en la columna housing_median_age en la 'mean' la media, o en las filas min y max que son la edad min y max de las casas

datos.hist()     #la funcion hist sirve para hacer histogramas

datos.hist(figsize=(15,8))          #el usar figsize nos permite mejorar el tamaño del histograma

datos.hist(figsize=(15,8), bins=30, edgecolor="black")       #el usar bins sirve para utilizar mas barras (mejorar particion) y edgecolor para usar un contorno negro

import pandas as pd
datos = pd.read_csv('housing.csv')

import seaborn as sb
sb.scatterplot(x='longitude', y='latitude', data=datos, hue='median_house_value', palette='coolwarm',
               s=datos['population']/100)

sb.scatterplot(x='longitude', y='latitude', data=datos[(datos.median_income > 14)], hue='median_house_value', palette='coolwarm')

datos.info()

datos_na = datos.dropna()

datos_na.info()

#Convertir la caracteristica categorica en numerica
#Proximidad al oceano
datos_na['ocean_proximity']

datos_na['ocean_proximity'].value_counts()
#convertir el dato en dato numerico "1,2,3,4,....

pd.get_dummies(datos['ocean_proximity'], dtype=int)

dummies =  pd.get_dummies(datos['ocean_proximity'], dtype=int)

#Usando la funcion join en nuestro conjunto "datos_na", podemos agregar los
#datos guardados en variable dummies
datos_na.join(dummies)

#ahora modificaremos el conjunto de datos_na con la funcion join
datos_na = datos_na.join(dummies)

# comprobaremos lo anterior usando funcion "head"
datos_na.head()

#ahora quitaremos la columna de "ocean_proximity".
#Para eso, usamos la funcion "drop", especificando la columna que queremos sacar
#Se coloca axis=1, para saber que tiene que quitar las columnas
datos_na.drop(['ocean_proximity'], axis=1)

#Ahora este conjunto, se coloca en el conjunto datos_na
datos_na = datos_na.drop(['ocean_proximity'], axis=1)

datos_na.head()

#Analisis con nuevas caracteristicas

#La funcion corr sirve para ver la correlacion entre los distintos datos
datos_na.corr()

#Usar Seaborn para hacer un heatmap
import seaborn as sb
sb.heatmap(datos_na.corr())

#A la funcion anterior, podemos agregar "annot" que significa anotacion,
#quedando
sb.heatmap(datos_na.corr(), annot=True)

#Se agrega una libreria para cambiar el tamaño de la grafica con "figure.figsize"
#Ocupando la funcion set de Seaborn
sb.set(rc={'figure.figsize': (15,8)})
#Ahora se cambio el color, usando cmap=YIGnBu
sb.heatmap(datos_na.corr(), annot=True, cmap='YlGnBu')

#Nos concentraremos en median_house_value
#Usando "sort_values" podemos ordenar estos datos en orden ascendente
#o descendente. En este caso, de forma descendiente
datos_na.corr()['median_house_value'].sort_values(ascending=False)

#Usamos la funcion "Query", que permite buscar bajo cierta condicion
#las filas que necesitamos. Además, nos dice cuantas filas y columnas
#cumplen esta condicion anterior

datos_na.query('housing_median_age > 50')

datos_na.query('median_house_value > 500000')

datos_na.query('median_income > 15')

datos_nue = datos_na[datos_na['housing_median_age'] < 50]

datos_nue = datos_na[datos_na['median_house_value'] < 500000]

datos_nue = datos_na[datos_na['median_income'] < 15]

datos_nue.info()

datos_nue.head()

datos_nue.corr()

sb.set(rc={'figure.figsize' : (15,8)})
sb.heatmap(datos_nue.corr(), annot=True, cmap='YlGnBu')

datos_nue.corr()['median_house_value'].sort_values(ascending=False)

datos_nue

sb.set(rc={'figure.figsize':(15,8)})
sb.heatmap(datos_nue.corr(), annot=True, cmap='YlGnBu')

X = datos_nue.drop(['median_house_value'], axis=1)
y = datos_nue['median_house_value']

from sklearn.model_selection import train_test_split

X_ent, X_pru, y_ent, y_pru = train_test_split(X, y, test_size=.2)

X_ent.shape

y_ent.shape

X_pru.shape

y_pru.shape

from sklearn.linear_model import LinearRegression
modelo  = LinearRegression()

modelo.fit(X_ent, y_ent)

predicciones = modelo.predict(X_pru)

predicciones

pd.DataFrame(predicciones)

comparativa = {'Prediccion': predicciones}
pd.DataFrame(comparativa)

comparativa = {'Prediccion': predicciones, 'Valor_real': y_pru}
pd.DataFrame(comparativa)

print(modelo.score(X_ent, y_ent))
print(modelo.score(X_pru, y_pru))

from sklearn.metrics import mean_squared_error
import numpy as np
mean_squared_error(y_pru, predicciones)

rmse = np.sqrt(mean_squared_error(y_pru, predicciones))

rmse

datos_nue.describe()

from sklearn.preprocessing import StandardScaler

Scaler = StandardScaler()

X_ent_escal = Scaler.fit_transform(X_ent)
y_ent_escal = Scaler.fit_transform(X_pru)

X_ent

X_ent_escal

pd.DataFrame(X_ent_escal)